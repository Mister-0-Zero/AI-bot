import torch

from app.core.logging_config import get_logger
from app.core.vector_store import load_vector_db
from app.llm import get_model

logger = get_logger(__name__)
MAX_NEW_TOKENS = 120


def search_knowledge(query: str, k: int = 5) -> list[str]:
    db = load_vector_db()
    results = db.similarity_search(query, k=k)
    return [r.page_content for r in results]


def generate_reply(history: list[dict]) -> str:
    latest_user_input = next(
        (msg["text"] for msg in reversed(history) if msg["role"] == "user"), "..."
    )

    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞, –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: %s", latest_user_input)

    context_chunks = search_knowledge(latest_user_input)
    if context_chunks:
        context = "\n\n".join(context_chunks)
        system_prompt = (
            "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
            "–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤ Google –î–∏—Å–∫–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö, –∫—Ä–∞—Ç–∫–∏—Ö –∏ –ø–æ–Ω—è—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n\n"
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}"
        )
        logger.info("–î–æ–±–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç (%d —á–∞–Ω–∫–æ–≤)", len(context_chunks))
    else:
        system_prompt = (
            "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
            "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞."
        )
        logger.info("–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")

    # üí¨ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
    prompt = f"<|system|>\n{system_prompt}\n"
    for msg in history:
        prompt += f"<|{msg['role']}|>\n{msg['text'].strip()}\n"
    prompt += "<|assistant|>\n"

    logger.info("–ü—Ä–æ–º–ø—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: %s...", prompt)

    tokenizer, model = get_model()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=MAX_NEW_TOKENS,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            do_sample=False,
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = decoded.split("<|assistant|>")[-1].strip()
    for stop_token in ["<|user|>", "user:", "assistant:", "you:", "ai:"]:
        if stop_token in answer.lower():
            answer = answer.split(stop_token)[0].strip()
    logger.info("PROMPT:\n%s\n\nRESPONSE:\n%s", prompt, answer)
    logger.info("–û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: %s", answer or "[–ø—É—Å—Ç–æ]")
    return answer or "ü§ñ –ü–æ–∫–∞ –Ω–µ –∑–Ω–∞—é, –∫–∞–∫ –æ—Ç–≤–µ—Ç–∏—Ç—å."
