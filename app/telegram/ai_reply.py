import re
from typing import List

from app.ai.groq_config import chat_completion
from app.core.logging_config import get_logger
from app.core.vector_store import load_vector_db

logger = get_logger(__name__)
MAX_NEW_TOKENS = 350


def _translate_ru_to_en(text: str) -> str:
    """
    ĞĞ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹.
    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Groq â€” Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾ Ğ¸ 1-2 Ñ‚Ğ¾ĞºĞµĞ½Ğ°.
    """
    try:
        resp = chat_completion(
            [
                {"role": "system", "content": "Translate the text to English."},
                {"role": "user", "content": text},
            ],
            max_tokens=60,
            temperature=0.0,
        )
        return resp.strip()
    except Exception as e:
        logger.warning("Translation failed: %s", e)
        return ""


def search_knowledge(query: str, user_id: int, k: int = 8) -> List[str]:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.
    * Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ ÑĞ²Ğ½Ğ¾ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°ĞµÑ‚ Â«Ñ„Ğ°Ğ¹Ğ»(Ñ‹) Ğ¿Ğ¾ â€¦Â» â€” Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸.
    * Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ñ†ĞµĞ»Ğ¸ĞºĞ¾Ğ¼ Ğ½Ğ° ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ğµ, Ğ¸Ñ‰ĞµĞ¼ ĞµÑ‰Ñ‘ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ.
    """
    q = query.strip()
    words = q.split()
    if len(words) < 3:  # ÑĞµÑ€Ğ²Ğ¸Ñ-Ñ„Ñ€Ğ°Ğ·Ñ‹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ RAG
        return []

    # â”€â”€ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºÑƒ Ğ¸Ğ¼ĞµĞ½Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    m = re.search(r"(?:Ñ„Ğ°Ğ¹Ğ»(?:Ñ‹)? Ğ¿Ğ¾|files? (?:about|on))\s+([\w\-\.\s]+)", q, re.I)
    filename_hint = m.group(1).strip().lower() if m else None

    # â”€â”€ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (ru + en) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    queries = [q]
    if not re.search(r"[A-Za-z]", q):  # Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ğ°
        en = _translate_ru_to_en(q)
        if en:
            queries.append(en)

    db = load_vector_db()
    results = []

    # â”€â”€ Ğ´ĞµĞ»Ğ°ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for q_ in queries:
        docs_scores = db.similarity_search_with_relevance_scores(
            q_,
            k=k,
            filter={"user_id": user_id},
        )
        results.extend(docs_scores)

    # â”€â”€ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ° (ĞµÑĞ»Ğ¸ Ğ±Ñ‹Ğ» Ñ…Ğ¸Ğ½Ñ‚) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    uniq: dict[str, float] = {}  # id -> best_score
    docs: dict[str, str] = {}  # id -> text

    for doc, score in results:
        fname = doc.metadata.get("file_name", "").lower()
        if filename_hint and filename_hint not in fname:
            continue  # Ğ½Ğµ Ñ‚Ğ¾Ñ‚ Ñ„Ğ°Ğ¹Ğ»
        doc_id = doc.metadata["file_id"] + doc.page_content[:20]
        best = uniq.get(doc_id, 0)
        if score > best:
            uniq[doc_id] = score
            docs[doc_id] = doc.page_content

    # â”€â”€ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    min_score = 0.65 if len(words) == 3 else 0.55 if len(words) == 4 else 0.35

    # Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑˆĞµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°
    return [docs[d] for d, s in uniq.items() if s >= min_score][:k]


MAX_MSGS = 6


def generate_reply(history: list[dict], user_id: int) -> str:
    latest_user_input = (
        next((m["text"] for m in reversed(history) if m["role"] == "user"), "...")
        .strip()
        .lower()
    )

    logger.info("ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´: %s", latest_user_input)

    # â”€â”€ RAG-ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    context_chunks = search_knowledge(latest_user_input, user_id)
    if context_chunks:
        context_block = "ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:\n" + "\n\n".join(context_chunks)
        system_prompt = (
            "Ğ¢Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Google Ğ”Ğ¸ÑĞºĞ° "
            "Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ…, ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².\n\n" + context_block
        )
    else:
        system_prompt = "Ğ¢Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°."

    # â”€â”€ messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    messages = [{"role": "system", "content": system_prompt}]
    for msg in history[-MAX_MSGS:]:
        messages.append({"role": msg["role"], "content": msg["text"].strip()})

    logger.info("Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Groq: %s", messages)

    # â”€â”€ Ğ²Ñ‹Ğ·Ğ¾Ğ² Groq â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        answer = chat_completion(messages, max_tokens=350, temperature=0.7)
    except Exception as e:
        logger.exception("Groq error: %s", e)
        return "âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ·."

    logger.info("ĞÑ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ Groq: %s", answer)

    return answer or "ğŸ¤– ĞŸĞ¾ĞºĞ° Ğ½Ğµ Ğ·Ğ½Ğ°Ñ, ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ."
