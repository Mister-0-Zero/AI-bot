from app.llm import get_model
import torch
from app.core.logging_config import get_logger

logger = get_logger(__name__)
MAX_NEW_TOKENS = 120

def generate_reply(user_text: str) -> str:
    logger.info("–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: %s", user_text)

    tokenizer, model = get_model()

    prompt = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_text}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
    inputs = tokenizer(prompt, return_tensors="pt")

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=MAX_NEW_TOKENS,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≤—ã—Ä–µ–∑–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    if "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:" in decoded:
        answer = decoded.split("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:")[-1].strip()
    else:
        answer = decoded.strip()

    logger.info("–û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: %s", answer or '[–ø—É—Å—Ç–æ]')
    return answer or "ü§ñ –ü–æ–∫–∞ –Ω–µ –∑–Ω–∞—é, –∫–∞–∫ –æ—Ç–≤–µ—Ç–∏—Ç—å."