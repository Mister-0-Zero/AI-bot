from app.ai.groq_config import chat_completion
from app.core.logging_config import get_logger
from app.core.vector_store import load_vector_db

logger = get_logger(__name__)
MAX_NEW_TOKENS = 350


def search_knowledge(
    query: str,
    user_id: int,
    k: int = 8,
) -> list[str]:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‡Ğ°Ğ½ĞºĞ¸ Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.
    Ğ”Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ( < 3 ÑĞ»Ğ¾Ğ²) Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ¸Ñ‰ĞµÑ‚.
    """
    q = query.strip()
    words = q.split()

    # âš‘ 1. ĞœĞµĞ»ĞºĞ¸Ğµ ÑĞµÑ€Ğ²Ğ¸Ñ-Ñ„Ñ€Ğ°Ğ·Ñ‹ Ğ²Ğ¾Ğ¾Ğ±Ñ‰Ğµ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
    if len(words) < 3:
        return []

    db = load_vector_db()

    # âš‘ 2. Ğ¡Ñ€Ğ°Ğ·Ñƒ Ğ±ĞµÑ€Ñ‘Ğ¼ (Document, score)
    docs_scores = db.similarity_search_with_relevance_scores(
        q,
        k=k,
        filter={"user_id": user_id},  # Ğ¸Ñ‰ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ€ĞµĞ´Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
    )

    # âš‘ 3. Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³: Ñ‡ĞµĞ¼ ĞºĞ¾Ñ€Ğ¾Ñ‡Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ñ‚ĞµĞ¼ Ğ²Ñ‹ÑˆĞµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³
    min_score = 0.65 if len(words) == 3 else 0.55 if len(words) == 4 else 0.45

    return [doc.page_content for doc, score in docs_scores if score >= min_score]


MAX_MSGS = 6


def generate_reply(history: list[dict], user_id: int) -> str:
    latest_user_input = (
        next((m["text"] for m in reversed(history) if m["role"] == "user"), "...")
        .strip()
        .lower()
    )

    # â”€â”€ RAG-ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    context_chunks = search_knowledge(latest_user_input, user_id)
    if context_chunks:
        context_block = "ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:\n" + "\n\n".join(context_chunks)
        system_prompt = (
            "Ğ¢Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Google Ğ”Ğ¸ÑĞºĞ° "
            "Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ…, ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².\n\n" + context_block
        )
    else:
        system_prompt = "Ğ¢Ñ‹ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°."
    # â”€â”€ messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    messages = [{"role": "system", "content": system_prompt}]
    for msg in history[-MAX_MSGS:]:
        messages.append({"role": msg["role"], "content": msg["text"].strip()})

    logger.info("System prompt Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½")

    # â”€â”€ Ğ²Ñ‹Ğ·Ğ¾Ğ² Groq â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        answer = chat_completion(messages, max_tokens=350, temperature=0.7)
    except Exception as e:
        logger.exception("Groq error: %s", e)
        return "âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ·."

    return answer or "ğŸ¤– ĞŸĞ¾ĞºĞ° Ğ½Ğµ Ğ·Ğ½Ğ°Ñ, ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ."
