import torch

from app.core.logging_config import get_logger
from app.core.vector_store import load_vector_db
from app.llm import get_model

logger = get_logger(__name__)
MAX_NEW_TOKENS = 120


def search_knowledge(query: str, k: int = 5) -> list[str]:
    db = load_vector_db()
    results = db.similarity_search(query, k=k)
    return [r.page_content for r in results]


def generate_reply(history: list[str]) -> str:
    latest_user_input = history[-1] if history else "..."

    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞, –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: %s", latest_user_input)

    # üîç –ü–æ–∏—Å–∫ –∑–Ω–∞–Ω–∏–π
    context_chunks = search_knowledge(latest_user_input)
    if context_chunks:
        context = "\n\n".join(context_chunks)
        system_prompt = (
            "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
            "–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤ Google –î–∏—Å–∫–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö, –∫—Ä–∞—Ç–∫–∏—Ö –∏ –ø–æ–Ω—è—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n\n"
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}"
        )
        logger.info("–î–æ–±–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç (%d —á–∞–Ω–∫–æ–≤)", len(context_chunks))
    else:
        system_prompt = (
            "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
            "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞."
        )
        logger.info("–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")

    # üí¨ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ TinyLlama
    prompt = f"<|system|>\n{system_prompt}\n"
    for i, message in enumerate(history):
        role = "user" if i % 2 == 0 else "assistant"
        prompt += f"<|{role}|>\n{message}\n"
    prompt += "<|assistant|>\n"

    logger.info("–ü—Ä–æ–º–ø—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: %s", prompt + "...")
    # üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    tokenizer, model = get_model()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=MAX_NEW_TOKENS,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # üí° –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ <|assistant|>
    answer = decoded.split("<|assistant|>")[-1].strip()

    logger.info(f"prompt: {prompt}")
    logger.info("–û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: %s", answer or "[–ø—É—Å—Ç–æ]")
    return answer or "ü§ñ –ü–æ–∫–∞ –Ω–µ –∑–Ω–∞—é, –∫–∞–∫ –æ—Ç–≤–µ—Ç–∏—Ç—å."
