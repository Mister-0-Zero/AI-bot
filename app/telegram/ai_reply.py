from app.llm import get_model
import torch

MAX_NEW_TOKENS = 120

async def generate_reply(user_text: str) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Ö–æ–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É.
    –†–∞–±–æ—Ç–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–æ —Å–∞–º–∞ –º–æ–¥–µ–ª—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è ‚Äì –ø–æ—ç—Ç–æ–º—É offload –≤ executor.
    """
    tok, model = get_model()
    prompt = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_text}\n–ú–æ–¥–µ–ª—å:"

    inputs = tok(prompt, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            pad_token_id=tok.eos_token_id,
            do_sample=True,
            top_p=0.9,
            temperature=0.8,
        )
    full_text = tok.decode(outputs[0], skip_special_tokens=True)
    # –±–µ—Ä—ë–º –≤—Å—ë –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–µ—Ç–∫–∏ ¬´–ú–æ–¥–µ–ª—å:¬ª
    answer = full_text.split("–ú–æ–¥–µ–ª—å:")[-1].strip()
    return answer or "ü§ñ ‚Ä¶"