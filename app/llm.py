from functools import lru_cache
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_ID = "sberbank-ai/rugpt3small_based_on_gpt2"
HF_CACHE_DIR = os.getenv("HF_HOME", "/mnt/models")

@lru_cache(maxsize=1)
def get_model():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ INT8-–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ,
    –∑–∞—Ç–µ–º –∫—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ø–∞–º—è—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞.
    """
    tok = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=HF_CACHE_DIR)

    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        cache_dir=HF_CACHE_DIR,
        low_cpu_mem_usage=True,         # —ç–∫–æ–Ω–æ–º–∏—è RAM
        torch_dtype=torch.float32,      # –±–µ–∑–æ–ø–∞—Å–Ω—ã–π CPU-—Ä–µ–∂–∏–º
    )

    # üí° –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ Linear-—Å–ª–æ—ë–≤ (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
    quant_model = torch.quantization.quantize_dynamic(
        base_model, {torch.nn.Linear}, dtype=torch.qint8
    )
    quant_model.eval()

    return tok, quant_model